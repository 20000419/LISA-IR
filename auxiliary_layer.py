#!/usr/bin/env python3
"""
AI Auxiliary Layer for Semantic Extraction

Generated by PROMPT-06 (Complete Version)
This module implements the AI auxiliary layer that interacts with local LLMs
to extract semantic hints from C code comments and documentation.

The auxiliary layer enhances the static analysis by leveraging AI models to
understand developer intent and extract precise API semantics from natural
language descriptions in comments.
"""

import json
import logging
import re
import requests
from typing import Dict, Any, List, Tuple, Optional
import time


# Default configuration for local LLM endpoint
DEFAULT_LLM_ENDPOINT = "http://192.168.137.1:6006/v1/chat/completions"
DEFAULT_TIMEOUT = 60  # seconds
DEFAULT_MAX_RETRIES = 3


class AuxiliaryLayer:
    """
    AI auxiliary layer for extracting semantic hints from C code comments.

    This class manages communication with local LLMs to extract precise semantic
    information about Python/C API functions from their comments and documentation.
    """

    def __init__(self,
                 llm_endpoint: str = DEFAULT_LLM_ENDPOINT,
                 timeout: int = DEFAULT_TIMEOUT,
                 max_retries: int = DEFAULT_MAX_RETRIES,
                 verbose: bool = False):
        """
        Initialize the auxiliary layer.

        Args:
            llm_endpoint: API endpoint for the local LLM
            timeout: Request timeout in seconds
            max_retries: Maximum number of retry attempts
            verbose: Enable verbose logging
        """
        self.llm_endpoint = llm_endpoint
        self.timeout = timeout
        self.max_retries = max_retries
        self.logger = self._setup_logger(verbose)

        self.logger.info(f"Initialized auxiliary layer with LLM endpoint: {llm_endpoint}")

    def _setup_logger(self, verbose: bool) -> logging.Logger:
        """Set up logging configuration."""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        logger.setLevel(logging.DEBUG if verbose else logging.INFO)
        return logger

    def _create_few_shot_prompt(self) -> str:
        """
        Create a few-shot learning prompt with examples.

        Returns:
            Few-shot prompt string
        """
        examples = [
            {
                "input": """
/*
 * Creates a new Python list object.
 * Returns a new reference or NULL on failure.
 */
PyObject* my_create_list(int size)""",
                "output": """
{{
    "my_create_list": {{
        "return_ref_type": "new_ref",
        "arg_ref_steal": {{}},
        "error_return": "NULL"
    }}
}}"""
            },
            {
                "input": """
/*
 * Appends an item to the list, stealing the reference.
 * Returns 0 on success, -1 on failure.
 */
int list_append_steal(PyObject* list, PyObject* item)""",
                "output": """
{{
    "list_append_steal": {{
        "return_ref_type": "none",
        "arg_ref_steal": {{
            "1": true
        }},
        "error_return": "-1"
    }}
}}"""
            },
            {
                "input": """
/*
 * Gets an item from the dictionary without changing reference count.
 * Returns borrowed reference, NULL if key not found.
 */
PyObject* dict_get_borrowed(PyObject* dict, const char* key)""",
                "output": """
{{
    "dict_get_borrowed": {{
        "return_ref_type": "borrowed_ref",
        "arg_ref_steal": {{}},
        "error_return": "NULL"
    }}
}}"""
            }
        ]

        prompt_parts = [
            "You are an expert in the Python/C API. Your task is to analyze C function signatures and their associated comments to extract semantic information about reference counting and error handling.",
            "",
            "You must respond ONLY with a valid JSON object containing the semantic information.",
            "",
            "The semantic information should include:",
            "- return_ref_type: \"new_ref\", \"borrowed_ref\", or \"none\"",
            "- arg_ref_steal: object mapping argument indices to true if they steal references",
            "- error_return: the value indicating failure (\"NULL\", \"-1\", etc.) or null",
            "",
            "Here are some examples:"
        ]

        for i, example in enumerate(examples, 1):
            prompt_parts.extend([
                f"Example {i}:",
                "Input:",
                "```c",
                example["input"].strip(),
                "```",
                "",
                "Output:",
                "```json",
                example["output"].strip(),
                "```",
                ""
            ])

        prompt_parts.extend([
            "Now analyze the following function and provide the semantic information:",
            "",
            "Input:",
            "```c",
            "{function_input}",
            "```",
            "",
            "Output:",
            "```json"
        ])

        return "\n".join(prompt_parts)

    def _extract_function_signatures_and_comments(self, c_code: str) -> List[Tuple[str, str, str]]:
        """
        Extract function signatures and their associated comments from C code.

        Args:
            c_code: C source code string

        Returns:
            List of tuples (function_name, signature, comment)
        """
        functions = []

        # Remove string literals to avoid false matches
        c_code_no_strings = re.sub(r'"[^"]*"', '""', c_code)

        # Find all multiline comments
        comment_pattern = r'/\*(.*?)\*/'
        comments = []
        for match in re.finditer(comment_pattern, c_code_no_strings, re.DOTALL):
            comment_text = match.group(1).strip()
            comment_start = match.start()
            comment_end = match.end()
            comments.append((comment_start, comment_end, comment_text))

        # Find all function definitions
        function_pattern = r'([a-zA-Z_][a-zA-Z0-9_\s\*]*?)\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\([^)]*\)\s*\{'

        for match in re.finditer(function_pattern, c_code_no_strings):
            func_start = match.start()
            func_name = match.group(2)
            return_type = match.group(1).strip()
            params_start = c_code_no_strings.find('(', func_start)
            params_end = c_code_no_strings.find(')', params_start)

            if params_start != -1 and params_end != -1:
                params = c_code_no_strings[params_start:params_end+1]
                signature = f"{return_type} {func_name}{params}"
            else:
                signature = f"{return_type} {func_name}()"

            # Find the most recent comment before this function
            associated_comment = ""
            for comment_start, comment_end, comment_text in reversed(comments):
                if comment_end < func_start:
                    # Check if the comment is reasonably close to the function (within 100 characters)
                    if func_start - comment_end < 100:
                        associated_comment = comment_text
                    break

            functions.append((func_name, signature, associated_comment))

        return functions

    def _create_function_prompt(self, signature: str, comment: str, few_shot_template: str) -> str:
        """
        Create a prompt for analyzing a specific function.

        Args:
            signature: Function signature
            comment: Associated comment
            few_shot_template: Few-shot learning template

        Returns:
            Complete prompt string
        """
        function_input = f"/*\n * {comment}\n */\n{signature}" if comment else signature
        return few_shot_template.format(function_input=function_input)

    def _call_llm_api(self, prompt: str) -> Optional[str]:
        """
        Call the local LLM API to extract semantic information.

        Args:
            prompt: The prompt to send to the LLM

        Returns:
            LLM response as string, or None if failed
        """
        payload = {
            "model": "qwen3-32b-awq",  # Use the specified model
            "messages": [
                {
                    "role": "user",
                    "content": f"思{prompt}思"  # Add thinking chain markers
                }
            ],
            "temperature": 0.1,  # Low temperature for more deterministic output
            "stream": False
        }

        for attempt in range(self.max_retries):
            try:
                self.logger.debug(f"Calling LLM API (attempt {attempt + 1}/{self.max_retries})")

                response = requests.post(
                    self.llm_endpoint,
                    json=payload,
                    timeout=self.timeout,
                    headers={"Content-Type": "application/json"}
                )

                if response.status_code == 200:
                    response_data = response.json()

                    # Handle different API response formats
                    if "choices" in response_data and len(response_data["choices"]) > 0:
                        content = response_data["choices"][0]["message"]["content"]
                        self.logger.debug(f"LLM response received: {len(content)} characters")
                        return content
                    else:
                        self.logger.warning(f"Unexpected LLM API response format: {response_data}")

                else:
                    self.logger.warning(f"LLM API request failed with status {response.status_code}: {response.text}")

            except requests.exceptions.Timeout:
                self.logger.warning(f"LLM API request timed out (attempt {attempt + 1})")
            except requests.exceptions.ConnectionError:
                self.logger.warning(f"Failed to connect to LLM API at {self.llm_endpoint}")
                break  # Don't retry connection errors
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"LLM API request error: {e}")
            except json.JSONDecodeError as e:
                self.logger.warning(f"Failed to decode LLM API response: {e}")
            except Exception as e:
                self.logger.warning(f"Unexpected error calling LLM API: {e}")

            # Wait before retry
            if attempt < self.max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff

        return None

    def _parse_llm_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Parse the LLM response to extract semantic information.

        Args:
            response: Raw LLM response string

        Returns:
            Parsed semantic information dictionary, or None if parsing failed
        """
        if not response:
            return None

        try:
            # Remove thinking chain markers if present
            if response.startswith("思") and response.endswith("思"):
                response = response[1:-1].strip()

            # Try to extract JSON from the response
            # Look for JSON code blocks
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
            else:
                # Look for JSON object in the response
                json_start = response.find('{')
                if json_start != -1:
                    # Find the matching closing brace
                    brace_count = 0
                    for i in range(json_start, len(response)):
                        if response[i] == '{':
                            brace_count += 1
                        elif response[i] == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                json_str = response[json_start:i+1]
                                break
                    else:
                        # If we didn't find a matching brace, take everything from first {
                        json_str = response[json_start:].strip()
                else:
                    # If no JSON object found, try to parse the whole response
                    json_str = response.strip()

            # Clean up common issues
            json_str = json_str.replace('\n', ' ').replace('\r', '')
            json_str = re.sub(r'\s+', ' ', json_str).strip()

            # Parse the JSON
            semantic_info = json.loads(json_str)

            # Validate the semantic information structure
            if not isinstance(semantic_info, dict):
                self.logger.warning("LLM response is not a dictionary")
                return None

            # Validate each function's semantic info
            validated_info = {}
            for func_name, info in semantic_info.items():
                if not isinstance(info, dict):
                    continue

                validated_func_info = {}

                # Validate return_ref_type
                if "return_ref_type" in info:
                    if info["return_ref_type"] in ["new_ref", "borrowed_ref", "none"]:
                        validated_func_info["return_ref_type"] = info["return_ref_type"]
                    else:
                        self.logger.warning(f"Invalid return_ref_type for {func_name}: {info['return_ref_type']}")

                # Validate arg_ref_steal
                if "arg_ref_steal" in info:
                    if isinstance(info["arg_ref_steal"], dict):
                        validated_func_info["arg_ref_steal"] = info["arg_ref_steal"]
                    else:
                        self.logger.warning(f"Invalid arg_ref_steal for {func_name}: must be a dictionary")

                # Validate error_return
                if "error_return" in info:
                    validated_func_info["error_return"] = info["error_return"]

                if validated_func_info:
                    validated_info[func_name] = validated_func_info

            return validated_info if validated_info else None

        except json.JSONDecodeError as e:
            self.logger.warning(f"Failed to parse LLM JSON response: {e}")
            self.logger.debug(f"Response content: {response}")
            return None
        except Exception as e:
            self.logger.warning(f"Error parsing LLM response: {e}")
            return None

    def extract_semantic_hints(self, c_code: str) -> Dict[str, Dict[str, Any]]:
        """
        Extract semantic hints from C code using AI assistance.

        Args:
            c_code: C source code string

        Returns:
            Dictionary mapping function names to their semantic information
        """
        self.logger.info("Starting AI-assisted semantic extraction")

        # Extract functions and their comments
        functions = self._extract_function_signatures_and_comments(c_code)
        self.logger.info(f"Found {len(functions)} functions with comments")

        if not functions:
            self.logger.info("No functions with comments found")
            return {}

        # Create few-shot learning template
        few_shot_template = self._create_few_shot_prompt()

        # Extract semantic information for each function
        all_semantic_info = {}

        for i, (func_name, signature, comment) in enumerate(functions, 1):
            self.logger.info(f"Processing function {i}/{len(functions)}: {func_name}")

            # Skip if comment is empty or too short
            if not comment or len(comment.strip()) < 10:
                self.logger.debug(f"Skipping {func_name}: no meaningful comment")
                continue

            # Create prompt for this function
            prompt = self._create_function_prompt(signature, comment, few_shot_template)

            # Call LLM API
            response = self._call_llm_api(prompt)
            if not response:
                self.logger.warning(f"Failed to get LLM response for {func_name}")
                continue

            # Parse response
            semantic_info = self._parse_llm_response(response)
            if semantic_info:
                all_semantic_info.update(semantic_info)
                self.logger.info(f"Successfully extracted semantic info for {func_name}")
            else:
                self.logger.warning(f"Failed to parse semantic info for {func_name}")

        self.logger.info(f"AI extraction completed: semantic info for {len(all_semantic_info)} functions")
        return all_semantic_info


def extract_semantic_hints(c_code: str,
                          llm_endpoint: str = DEFAULT_LLM_ENDPOINT,
                          timeout: int = DEFAULT_TIMEOUT,
                          max_retries: int = DEFAULT_MAX_RETRIES,
                          verbose: bool = False) -> Dict[str, Dict[str, Any]]:
    """
    Convenience function to extract semantic hints from C code.

    Args:
        c_code: C source code string
        llm_endpoint: API endpoint for the local LLM
        timeout: Request timeout in seconds
        max_retries: Maximum number of retry attempts
        verbose: Enable verbose logging

    Returns:
        Dictionary mapping function names to their semantic information
    """
    auxiliary = AuxiliaryLayer(
        llm_endpoint=llm_endpoint,
        timeout=timeout,
        max_retries=max_retries,
        verbose=verbose
    )
    return auxiliary.extract_semantic_hints(c_code)


def test_llm_connection(llm_endpoint: str = DEFAULT_LLM_ENDPOINT,
                       timeout: int = 10) -> bool:
    """
    Test connection to the local LLM endpoint.

    Args:
        llm_endpoint: API endpoint for the local LLM
        timeout: Request timeout in seconds

    Returns:
        True if connection is successful, False otherwise
    """
    try:
        payload = {
            "model": "qwen3-32b-awq",
            "messages": [{"role": "user", "content": "思Hello思"}],
            "temperature": 0.1,
            "max_tokens": 10
        }

        response = requests.post(
            llm_endpoint,
            json=payload,
            timeout=timeout,
            headers={"Content-Type": "application/json"}
        )

        if response.status_code == 200:
            print(f"[OK] Successfully connected to LLM at {llm_endpoint}")
            return True
        else:
            print(f"[ERROR] LLM API returned status {response.status_code}: {response.text}")
            return False

    except requests.exceptions.ConnectionError:
        print(f"[ERROR] Failed to connect to LLM at {llm_endpoint}")
        print("  Make sure your local LLM server is running and accessible")
        return False
    except Exception as e:
        print(f"[ERROR] Error testing LLM connection: {e}")
        return False


def main():
    """Command-line interface for testing the auxiliary layer."""
    import argparse
    import sys
    import os

    parser = argparse.ArgumentParser(
        description="Test the AI auxiliary layer for semantic extraction"
    )
    parser.add_argument(
        "c_file",
        nargs="?",
        help="C source file to analyze"
    )
    parser.add_argument(
        "--endpoint",
        default=DEFAULT_LLM_ENDPOINT,
        help=f"LLM API endpoint (default: {DEFAULT_LLM_ENDPOINT})"
    )
    parser.add_argument(
        "--test-connection",
        action="store_true",
        help="Test connection to LLM endpoint"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=DEFAULT_TIMEOUT,
        help=f"Request timeout in seconds (default: {DEFAULT_TIMEOUT})"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )

    args = parser.parse_args()

    if args.test_connection:
        success = test_llm_connection(args.endpoint, args.timeout)
        sys.exit(0 if success else 1)

    if not args.c_file:
        parser.error("C source file is required (unless --test-connection is used)")

    if not os.path.exists(args.c_file):
        print(f"Error: File not found: {args.c_file}")
        sys.exit(1)

    try:
        with open(args.c_file, 'r', encoding='utf-8') as f:
            c_code = f.read()

        print(f"Analyzing C file: {args.c_file}")
        print(f"Using LLM endpoint: {args.endpoint}")

        semantic_hints = extract_semantic_hints(
            c_code,
            llm_endpoint=args.endpoint,
            timeout=args.timeout,
            verbose=args.verbose
        )

        if semantic_hints:
            print(f"\nExtracted semantic information for {len(semantic_hints)} functions:")
            print(json.dumps(semantic_hints, indent=2, ensure_ascii=False))
        else:
            print("No semantic information extracted")

    except Exception as e:
        print(f"Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()